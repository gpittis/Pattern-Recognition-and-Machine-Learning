{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c801c22c-cead-4bdf-9229-d24ace15e271",
   "metadata": {},
   "source": [
    "# Εργασία στην Αναγνώρηση Προτύπων και Μηχανική Μάθηση\n",
    "## Μέρος Δ\n",
    "## ΕΠΙΜΕΛΕΙΑ :\n",
    "* Ομάδα 25\n",
    "* Γουρδομιχάλης Αναστάσιος 10333\n",
    "* Πίττης Γεώργιος 10586"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3036d6-0469-479d-9fc9-001cc9b3d7cb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22478ec8-b6e0-4bfd-b7ad-c0a3e8e205c9",
   "metadata": {},
   "source": [
    "# ΜΕΡΟΣ Δ\n",
    "\n",
    "**Η λογική που ακολουθήσαμε στο θέμα Δ είναι η εξής:**\n",
    "\n",
    "### Χωρισμός του συνολικού training set:\n",
    "   - Χωρίσαμε το συνολικό training set των 8743 δειγμάτων σε 2 υποσύνολα:\n",
    "     - **Σύνολο εκπαίδευσης** [ X_train, y_train ] που είναι το 80% του συνολικού training set.\n",
    "     - **Validation set** [ X_val, y_val ] που ταυτίζεται με το 20% του συνολικού training set.\n",
    "   - Τα **X_train**, **X_val** περιέχουν τα 224 χαρακτηριστικά (features) των δειγμάτων.\n",
    "   - Τα **y_train**, **y_val** περιέχουν τις ετικέτες (labels) των δειγμάτων.\n",
    "\n",
    "### Manual hyperparameter tuning για κάθε αλγόριθμο ταξινόμησης:\n",
    "   - Εφαρμόσαμε manual hyperparameter tuning για τους παρακάτω αλγορίθμους ταξινόμησης:\n",
    "     - Random Forest (Τυχαίο Δάσος)\n",
    "     - SVM (Support Vector Machine)\n",
    "     - Gaussian Naive Bayes\n",
    "     - MLP (Multilayer Perceptron)\n",
    "     - k-Nearest Neighbors (KNN)\n",
    "     - Decision Tree (Δέντρο Απόφασης)\n",
    "     - Multinomial Logistic Regression (Λογιστική Παλινδρόμηση για Πολλαπλές Κλάσεις)\n",
    "   - Στη διαδικασία του tuning, εκπαιδεύσαμε διάφορα μοντέλα από κάθε αλγόριθμο ταξινόμησης στο [ X_train, y_train ] και τα τεστάραμε στο X_val.\n",
    "   - Με βάση την ακρίβεια (accuracy) στο validation set, επιλέξαμε το καλύτερο μοντέλο για κάθε αλγόριθμο.\n",
    "\n",
    "### Επιλογή του συνολικού καλύτερου μοντέλου:\n",
    "   - Αφού ολοκληρώσαμε το tuning για όλους τους αλγορίθμους, επιλέξαμε το συνολικό καλύτερο μοντέλο.\n",
    "\n",
    "### Βελτίωση απόδοσης με PCA (Principal Component Analysis):\n",
    "   - Για να βελτιώσουμε την απόδοση του συνολικού καλύτερου μοντέλου, χρησιμοποιήσαμε τη μέθοδο **PCA**, διατηρώντας 139 χαρακτηριστικά (principal   components).\n",
    "   - Ο αριθμός αυτός επιλέχθηκε μετά από διαδικασία tuning στο εύρος [1, 224].\n",
    "   - Εφαρμόσαμε τη μέθοδο PCA στο 80% του training set και εκπαιδεύσαμε ξανά το μοντέλο. Ως αποτέλεσμα, η ακρίβεια του μοντέλου στο validation set αυξήθηκε.\n",
    "\n",
    "### Επανεκπαίδευση με PCA στο συνολικό training set:\n",
    "   - Για να εκμεταλλευτούμε τη βελτίωση αυτή, αποφασίσαμε να επανεκπαιδεύσουμε το καλύτερο μοντέλο με την τεχνική PCA στο συνολικό training set.\n",
    "\n",
    "### Προβλέψεις του τελικού εκπαιδευμένου μοντέλου στο test set:\n",
    "   - Το τελικό εκπαιδευμένο μοντέλο πραγματοποιεί προβλέψεις στο **test set**.\n",
    "   - Έτσι, παράγεται το αρχείο **labels25.npy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0dfe57-bae3-4d79-831e-d7c3aa3722b5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d6989-ab34-4f0e-bd26-b20970ff9708",
   "metadata": {},
   "source": [
    "### Φόρτωση των δεδομένων και χωρισμός σε 80% training set και 20% validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84a33c2-4c74-4834-8016-3f1741d79472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Load the Training and Test Data\n",
    "train_data = pd.read_csv('datasetTV.csv', header=None)  # Training data\n",
    "test_data = pd.read_csv('datasetTest.csv', header=None)  # Test data\n",
    "\n",
    "# Preprocess the Data\n",
    "# Separate features (X) and target labels (y) in the training data\n",
    "X_train_full = train_data.iloc[:, :-1].values  # All columns except the last column (features)\n",
    "y_train_full = train_data.iloc[:, -1].values  # Last column (target labels)\n",
    "X_test = test_data.values  # Test data features (all columns)\n",
    "\n",
    "# Split the training data into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a97f38-3fc3-4c33-bf04-9b664cd91eb0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb0261-a2f8-46f1-8d96-75542a5bd88d",
   "metadata": {},
   "source": [
    "### Ακολουθεί η συνάρτηση που δημιουργήσαμε για να κάνουμε manual hyperparameter tuning για κάθε αλγόριθμο ταξινόμησης"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a130e54-4a14-46a3-a84d-40bc0f8f46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_hyperparameter_tuning(algorithms, param_grids, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for multiple machine learning models.\n",
    "\n",
    "    Parameters:\n",
    "    algorithms (dict): A dictionary of model names and their corresponding classifiers.\n",
    "    param_grids (dict): A dictionary containing hyperparameter grids for each model.\n",
    "    X_train (array): Training features.\n",
    "    y_train (array): Training labels.\n",
    "    X_val (array): Validation features.\n",
    "    y_val (array): Validation labels.\n",
    "\n",
    "    Returns:\n",
    "    dict: Best models, best accuracies, and best parameters for each model.\n",
    "    \"\"\"\n",
    "    # Dictionaries to store the best model, best accuracy, and best hyperparameters for each model.\n",
    "    best_model_for_all = {}\n",
    "    best_accuracy_for_all = {}\n",
    "    best_params_for_all = {}\n",
    "\n",
    "    # Perform hyperparameter tuning for each model using the defined grids\n",
    "    for model_name, model in algorithms.items():\n",
    "        print(f\"Training and evaluating {model_name}...\")\n",
    "\n",
    "        # Get the corresponding parameter grid\n",
    "        param_grid = param_grids[model_name]\n",
    "\n",
    "        # Generate all possible combinations of hyperparameters\n",
    "        param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "        best_accuracy = 0  # Initialize best accuracy for comparison\n",
    "        best_params = None  # Initialize best parameters for comparison\n",
    "\n",
    "        # Iterate over each hyperparameter combination\n",
    "        for params in param_combinations:\n",
    "            param_dict = dict(zip(param_grid.keys(), params))  # Create a dictionary of hyperparameters\n",
    "            model.set_params(**param_dict)  # Set the model's hyperparameters\n",
    "            model.fit(X_train, y_train)  # Train the model on the training set\n",
    "            y_val_pred = model.predict(X_val)  # Make predictions on the validation set\n",
    "            val_accuracy = accuracy_score(y_val, y_val_pred)  # Calculate the accuracy on validation set\n",
    "\n",
    "            # Update the best accuracy and parameters if the current model is better\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                best_model = model\n",
    "                best_params = param_dict\n",
    "\n",
    "        # Store the results for this model\n",
    "        best_model_for_all[model_name] = best_model\n",
    "        best_accuracy_for_all[model_name] = best_accuracy\n",
    "        best_params_for_all[model_name] = best_params\n",
    "\n",
    "        # Print results for this model\n",
    "        print(f\"Best model for {model_name} with accuracy: {best_accuracy * 100:.2f}% and parameters: {best_params}\\n\")\n",
    "\n",
    "    return best_model_for_all, best_accuracy_for_all, best_params_for_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cdc517-76f2-44ec-a589-4ddd0d4af019",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96e4af-9741-4f37-8bff-ddc77573e461",
   "metadata": {},
   "source": [
    "### **Όλοι οι συνδυασμοί υπερπαραμέτρων** που δοκιμάστηκαν με την manual_hyperparameter_tuning για τους αλγόριθμους ταξινόμησης είναι οι εξής:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3102fd59-7bfe-4aa0-8d27-f6aa1531549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grids for each model\n",
    "\n",
    "\n",
    "# All the hyperparameters combinations for Random Forest\n",
    "param_grid_rf_all = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'max_samples': [0.7, 1.0]\n",
    "}\n",
    "\n",
    "# All the hyperparameters combinations for SVM\n",
    "param_grid_svm_all = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'tol': [1e-9, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'coef0': [0.001, 0.01, 0.1, 0.2, 0.5, 1.0],\n",
    "    'class_weight': [None, 'balanced']\n",
    "\n",
    "}\n",
    "\n",
    "# All the hyperparameters combinations for Naive Bayes\n",
    "param_grid_nb_all = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "\n",
    "# All the hyperparameters combinations for MLP\n",
    "param_grid_mlp_all = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (200,), (300,), (400,), (500,), (600,), (700,),\n",
    "                           (100, 100), (200, 200), (300, 300), (400, 400), (500, 500), (600, 600),\n",
    "                           (700, 700), (800, 800), (900, 900),\n",
    "                           (1000, 1000), (100, 100, 50), (100, 100, 100), (200, 200, 200), (300, 300, 200),\n",
    "                           (300, 300, 300), (400, 400, 400), (500, 500, 500), (600, 600, 600),\n",
    "                           (700, 700, 700), (800, 800, 800), (900, 900, 900), (1000, 1000, 1000)],\n",
    "    'activation': ['relu', 'logistic', 'tanh'],\n",
    "    'solver': ['adam', 'lbfgs', 'sgd'],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [2000],\n",
    "    'alpha': [0.00001, 0.0001, 0.001],\n",
    "    'epsilon': [1e-6, 1e-7, 1e-8, 1e-9, 1e-10]\n",
    "}\n",
    "\n",
    "# All the hyperparameters combinations for KNN\n",
    "param_grid_knn_all = {\n",
    "    'n_neighbors': [3, 5, 7, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# All the hyperparameters combinations for Decision Tree\n",
    "param_grid_dt_all = {\n",
    "    'max_depth': range(1, 50)\n",
    "}\n",
    "\n",
    "# All the hyperparameters combinations for Logistic Regression\n",
    "param_grid_lr_all = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear', 'saga', 'newton-cg'],\n",
    "    'max_iter': [1000, 2000],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'tol': [1e-4, 1e-3, 1e-2],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac96a95-0743-42ea-9af6-a1c79803d0ba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea5f08-90ca-4984-b9b5-b0d4780db89b",
   "metadata": {},
   "source": [
    "### Τα **καλύτερα μοντέλα για κάθε αλγόριθμο** προέκυψαν μετά από **αρκετές κλήσεις** της συνάρτησης manual_hyperparameter_tuning. \n",
    "### Οι κλήσεις αφορούσαν όλους τους παραπάνω συνδυασμούς υπερπαραμέτρων και τα αποτελέσματα είναι τα εξής:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676b8e07-4770-4d58-b058-cecd67f33543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperparameters of the best model for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [300],  # Number of trees in the forest\n",
    "    'max_depth': [40],  # Maximum depth of trees\n",
    "    'max_samples': [1.0]   # Proportion of the dataset used to train each tree\n",
    "}\n",
    "\n",
    "# The hyperparameters of the best model for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [1],  # Regularization strength\n",
    "    'gamma': ['scale'],  # Kernel coefficient\n",
    "    'kernel': ['poly'],  # Polynomial kernel\n",
    "    'tol': [1e-3],   # Tolerance for stopping criteria\n",
    "    'coef0': [0.2],  # Independent term in the kernel function\n",
    "    'class_weight': ['balanced']  # Adjust weights to handle class imbalance\n",
    "}\n",
    "\n",
    "# The hyperparameters of the best model for Naive Bayes\n",
    "param_grid_nb = {\n",
    "    'var_smoothing': [1e-9]   # Smoothing parameter to avoid zero variance\n",
    "}\n",
    "\n",
    "# The hyperparameters of the best model for MLP\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(300,)],  # One hidden layer with 300 neurons\n",
    "    'activation': ['relu'],  # Activation function\n",
    "    'solver': ['adam'],  # Solver for weight optimization\n",
    "    'learning_rate': ['constant'],  # Constant learning rate\n",
    "    'max_iter': [2000],  # Maximum number of iterations\n",
    "    'alpha': [0.0001],   # Regularization term\n",
    "    'epsilon': [1e-7]  # Tolerance for optimization\n",
    "}\n",
    "\n",
    "# The hyperparameters of the best model for KNN\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [5],  # Number of neighbors to use in classification\n",
    "    'weights': ['distance'],  # Use distance weighting for neighbors\n",
    "    'metric': ['euclidean']  # Use Euclidean distance metric\n",
    "}\n",
    "\n",
    "# The hyperparameters of the best model for Decision Tree\n",
    "param_grid_dt = {\n",
    "    'max_depth': [11]  # Maximum depth of the decision tree\n",
    "}\n",
    "\n",
    "# The hyperparameters of the best model for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.1],  # Regularization strength for logistic regression\n",
    "    'solver': ['lbfgs'],   # Solver for optimization\n",
    "    'max_iter': [1000],  # Maximum iterations for the solver\n",
    "    'class_weight': [None],  # No adjustment for class imbalance\n",
    "    'tol': [1e-2],  # Tolerance for optimization\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb15408-630e-45d0-af32-7f65072dbab4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e46f11-71b5-4d74-ab7f-27eff79d1517",
   "metadata": {},
   "source": [
    "### Στα πλαίσια του Μέρους Δ, καλούμε τη συνάρτηση manual_hyperparameter_tuning για να υπολογίσουμε την ακρίβεια στο validation set **ΜΟΝΟ** για το καλύτερο μοντέλο κάθε αλγορίθμου. \n",
    "\n",
    "### Η κλήση της συνάρτησης **για όλους τους δυνατούς συνδυασμούς υπερπαραμέτρων** σε κάθε αλγόριθμο, θα απαιτούσε υπερβολικά μεγάλο χρόνο εκτέλεσης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a78f3784-e3cd-4540-8006-5c22dd6a46a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Random Forest...\n",
      "Best model for Random Forest with accuracy: 81.93% and parameters: {'n_estimators': 300, 'max_depth': 40, 'max_samples': 1.0}\n",
      "\n",
      "Training and evaluating SVM...\n",
      "Best model for SVM with accuracy: 87.36% and parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'poly', 'tol': 0.001, 'coef0': 0.2, 'class_weight': 'balanced'}\n",
      "\n",
      "Training and evaluating Naive Bayes...\n",
      "Best model for Naive Bayes with accuracy: 69.87% and parameters: {'var_smoothing': 1e-09}\n",
      "\n",
      "Training and evaluating MLP...\n",
      "Best model for MLP with accuracy: 86.34% and parameters: {'hidden_layer_sizes': (300,), 'activation': 'relu', 'solver': 'adam', 'learning_rate': 'constant', 'max_iter': 2000, 'alpha': 0.0001, 'epsilon': 1e-07}\n",
      "\n",
      "Training and evaluating KNN...\n",
      "Best model for KNN with accuracy: 84.96% and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "\n",
      "Training and evaluating Decision Tree...\n",
      "Best model for Decision Tree with accuracy: 65.24% and parameters: {'max_depth': 11}\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Best model for Logistic Regression with accuracy: 78.67% and parameters: {'C': 0.1, 'solver': 'lbfgs', 'max_iter': 1000, 'class_weight': None, 'tol': 0.01}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the machine learning algorithms\n",
    "algorithms = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),  # Random Forest algorithm\n",
    "    \"SVM\": SVC(random_state=42),  # Support Vector Machine (SVM) algorithm\n",
    "    \"Naive Bayes\": GaussianNB(),  # Gaussian Naive Bayes algorithm\n",
    "    \"MLP\": MLPClassifier(random_state=42),  # Multilayer Perceptron (MLP) algorithm\n",
    "    \"KNN\": KNeighborsClassifier(),  # k-Nearest Neighbors (KNN) algorithm\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),  # Decision Tree algorithm\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42)  # Multinomial Logistic Regression algorithm\n",
    "}\n",
    "\n",
    "# Define the parameter grids ONLY for the BEST MODEL of each algorithm.\n",
    "param_grids = {\n",
    "    \"Random Forest\": param_grid_rf,  # Parameter grid for Random Forest\n",
    "    \"SVM\": param_grid_svm,  # Parameter grid for SVM\n",
    "    \"Naive Bayes\": param_grid_nb,  # Parameter grid for Gaussian Naive Bayes\n",
    "    \"MLP\": param_grid_mlp,  # Parameter grid for MLP\n",
    "    \"KNN\": param_grid_knn,  # Parameter grid for KNN\n",
    "    \"Decision Tree\": param_grid_dt,  # Parameter grid for Decision Tree\n",
    "    \"Logistic Regression\": param_grid_lr  # Parameter grid for Multinomial Logistic Regression\n",
    "}\n",
    "\n",
    "# Call the manual_hyperparameter_tuning function.\n",
    "# It returns the best model, best accuracy, and best parameters for each algorithm.\n",
    "best_model_for_all, best_accuracy_for_all, best_params_for_all = manual_hyperparameter_tuning(algorithms,\n",
    "                                                                                              param_grids,\n",
    "                                                                                              X_train,\n",
    "                                                                                              y_train,\n",
    "                                                                                              X_val,\n",
    "                                                                                              y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cab58a-44c3-4019-8381-4c5263d92a40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70dffb-b9ee-4441-9384-f04787c87aba",
   "metadata": {},
   "source": [
    "### **Επιλογή του συνολικού καλύτερου μοντέλου**: Επιλέγεται το μοντέλο με την καλύτερη ακρίβεια στο validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34da246b-8767-40cd-8b51-c77938406dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Final Best Model is SVM with best parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'poly', 'tol': 0.001, 'coef0': 0.2, 'class_weight': 'balanced'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the final best model with the highest accuracy in validation set.\n",
    "best_model_name = max(best_accuracy_for_all, key=best_accuracy_for_all.get)\n",
    "best_model = best_model_for_all[best_model_name]\n",
    "best_params = best_params_for_all[best_model_name]\n",
    "\n",
    "print(f\"The Final Best Model is {best_model_name} with best parameters: {best_params}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572bc889-280a-4cd3-afcc-5ebca87703c9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f71c84-4cf0-4555-97c8-7d6941500ef9",
   "metadata": {},
   "source": [
    "### **Βελτίωση απόδοσης με PCA (Principal Component Analysis)**: Εφαρμόσαμε τη μέθοδο PCA στο 80% του training set και εκπαιδεύσαμε ξανά το συνολικό καλύτερο μοντέλο. \n",
    "### **Αποτέλεσμα**: Η ακρίβεια του μοντέλου στο validation set αυξήθηκε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "008e85f4-d12d-4d55-a4c6-c6f9de1abb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(n_components=139)\n",
      "Training the best model (SVM) on the training set with PCA...\n",
      "\n",
      "Validation Accuracy for SVM with PCA: 88.56%\n",
      "\n",
      "Improvement of accuracy with PCA : 88.56 - 87.36 = 1.20%\n",
      "\n",
      "So the PCA method trains better the best model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA (Principal Component Analysis) to reduce the dimensionality of the data\n",
    "pca = PCA(n_components=139)  # Use 139 principal components\n",
    "print(pca)\n",
    "X_pca = pca.fit_transform(X_train_full)  # Apply PCA on the entire dataset\n",
    "\n",
    "# Split the PCA-transformed data into training and validation sets (80% train set and 20% validation set)\n",
    "X_train_pca, X_val_pca, y_train_pca, y_val_pca = train_test_split(X_pca, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the best model on the 80% of the total training set with PCA applied\n",
    "print(f\"Training the best model ({best_model_name}) on the training set with PCA...\\n\")\n",
    "\n",
    "best_model.set_params(**best_params)\n",
    "best_model.fit(X_train_pca, y_train_pca)  # Train the model on the PCA-transformed training set\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_predictions = best_model.predict(X_val_pca)  # Make predictions on the validation set\n",
    "val_accuracy = accuracy_score(y_val_pca, val_predictions)  # Calculate the validation accuracy\n",
    "\n",
    "best_model_accuracy_before_pca = best_accuracy_for_all[best_model_name]\n",
    "improvement = (val_accuracy * 100) - (best_model_accuracy_before_pca * 100)\n",
    "print(f\"Validation Accuracy for {best_model_name} with PCA: {val_accuracy * 100:.2f}%\\n\")\n",
    "print(f\"Improvement of accuracy with PCA : {val_accuracy * 100:.2f} - {best_model_accuracy_before_pca * 100:.2f} = \"\n",
    "      f\"{improvement:.2f}%\\n\")\n",
    "\n",
    "# We notice that the PCA trains our best model in a better way for this specific training set.\n",
    "# Because the validation accuracy is better with PCA.\n",
    "print(f\"So the PCA method trains better the best model\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96654c-f2cb-443f-931d-f10caf460a20",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95519d3f-f6dd-4e38-82b5-fe43af026a27",
   "metadata": {},
   "source": [
    "### Επανεκπαίδευση του καλύτερου μοντέλου με PCA στο **συνολικό training set**.\n",
    "### Το τελικό εκπαιδευμένο μοντέλο πραγματοποιεί προβλέψεις στο test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a81baf1-1175-4d26-91b2-b83ec0622186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-training the best model (SVM) on the entire training set with PCA...\n",
      "Test Predictions using SVM: [1 4 5 ... 3 5 1]\n",
      "Number of labels predicted: 6955\n"
     ]
    }
   ],
   "source": [
    "# Re-train the best model using the entire training set (with PCA applied)\n",
    "print(f\"Re-training the best model ({best_model_name}) on the entire training set with PCA...\")\n",
    "\n",
    "# Apply PCA on the entire dataset again (including the validation set, as it's already been split)\n",
    "X_train_full_pca = pca.transform(X_train_full)  # Transform the entire dataset with the same PCA transformation\n",
    "best_model.fit(X_train_full_pca, y_train_full)  # Train the model on the full PCA-transformed training set\n",
    "\n",
    "# Make predictions on the test set (with PCA applied)\n",
    "X_test_pca = pca.transform(X_test)  # Apply PCA transformation to the test set\n",
    "test_predictions = best_model.predict(X_test_pca)  # Predict on the PCA-transformed test set\n",
    "\n",
    "print(f\"Test Predictions using {best_model_name}: {test_predictions}\")\n",
    "print(f\"Number of labels predicted: {len(test_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064008f0-0544-473b-b099-81cbcf0616a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f18a4f-35b6-431c-a1a5-efe9c4ea2196",
   "metadata": {},
   "source": [
    "### Αποθήκευση των προβλέψεων στο labels25.npy και έλεγχος της εγκυρότητας των αποτελεσμάτων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eae4c82-4cc7-4084-8301-febff211da35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions have been saved to 'labels25.npy'.\n",
      "Labels loaded successfully! Number of labels: 6955\n",
      "Shape of the labels array: (6955,)\n",
      "The labels are from 1 to 5.\n",
      "All labels (1 to 5) appear at least once.\n"
     ]
    }
   ],
   "source": [
    "# test_predictions is the array with our best model predictions on test set.\n",
    "if len(test_predictions) == 6955:  # Check if the number of predictions is correct\n",
    "    # Save the predictions as a NumPy file\n",
    "    np.save(\"labels25.npy\", np.array(test_predictions))\n",
    "    print(f\"Predictions have been saved to 'labels25.npy'.\")\n",
    "else:\n",
    "    print(f\"Error: The number of test predictions is {len(test_predictions)}, but it should be 6955.\")\n",
    "\n",
    "# Check the .npy file\n",
    "# Load the labels from the .npy file\n",
    "try:\n",
    "    labels = np.load('labels25.npy')  # Load the saved predictions from the .npy file\n",
    "    print(f\"Labels loaded successfully! Number of labels: {len(labels)}\")\n",
    "    print(f\"Shape of the labels array: {labels.shape}\")  # Print the shape of the labels array\n",
    "\n",
    "    # Check if the labels are between 1 and 5 and each label appears at least once\n",
    "    unique_labels = np.unique(labels)  # Get unique labels from the predictions\n",
    "\n",
    "    # Check if the unique labels are between 1 and 5\n",
    "    if np.all(np.isin(unique_labels, [1, 2, 3, 4, 5])):\n",
    "        print(\"The labels are from 1 to 5.\")\n",
    "    else:\n",
    "        print(f\"Error: The labels contain values outside of 1 to 5: {unique_labels}\")\n",
    "\n",
    "    # Check if each label appears at least once\n",
    "    label_counts = {label: np.sum(labels == label) for label in [1, 2, 3, 4, 5]}\n",
    "    missing_labels = [label for label, count in label_counts.items() if count == 0]\n",
    "\n",
    "    if not missing_labels:\n",
    "        print(\"All labels (1 to 5) appear at least once.\")\n",
    "    else:\n",
    "        print(f\"Error: The following labels are missing in the predictions: {missing_labels}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'labels25.npy' could not be found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450942d-425e-4c04-b5b2-415a49714ad8",
   "metadata": {},
   "source": [
    "### Τέλος, μια γενική παρατήρηση για την εκπαίδευση των μοντέλων στο Μέρος Δ:\n",
    "#### Χρησιμοποιούμε ένα σταθερό 20% του συνόλου των 8743 δειγμάτων για την αξιολόγηση (validation) σε κάθε εκτέλεση, με στόχο την εξοικονόμηση χρόνου. Έχουμε διαπιστώσει ότι τα αποτελέσματα που προκύπτουν με αυτήν τη μέθοδο είναι συγκρίσιμα με εκείνα που προκύπτουν από το cross-validation, το οποίο εξετάζει διαφορετικά υποσύνολα του dataset. Επομένως, θεωρούμε ότι το σταθερό 20% είναι επαρκές για την αξιόπιστη αξιολόγηση των μοντέλων μας. Αυτή η προσέγγιση διασφαλίζει τη συνέπεια στη σύγκριση διαφορετικών μοντέλων, ενώ παράλληλα μειώνει τον κίνδυνο overfitting ή underfitting στο τελικό test set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
